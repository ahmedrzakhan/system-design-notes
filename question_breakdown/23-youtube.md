# YouTube System Design Interview Guide

## ğŸ“‹ Overview

YouTube is a video-sharing platform - the second most visited website in the world. This design has overlap with file storage systems (like Dropbox) but focuses on video streaming complexities.

## ğŸ¯ Functional Requirements

### Core Requirements

1. **Upload videos** - Users can upload videos
2. **Watch/stream videos** - Users can stream videos

### Below the Line (Out of Scope)

- View video metadata (view counts, descriptions)
- Search for videos
- Comment on videos
- Recommendations
- Channel management
- Subscriptions

## âš¡ Non-Functional Requirements

### Critical Requirements

1. **High availability** (prioritize over consistency)
2. **Support large videos** (10+ GB)
3. **Low latency streaming** (even in low bandwidth)
4. **High scale**:
   - ~1M videos uploaded/day
   - 100M videos watched/day
5. **Resumable uploads**

### Out of Scope

- Content moderation
- Bot protection
- Monitoring/alerting

## ğŸ—ï¸ Core Entities

```mermaid
erDiagram
    User ||--o{ Video : uploads
    Video ||--|| VideoMetadata : has
    VideoMetadata {
        string videoId
        string uploaderId
        string name
        string description
        json chunks
        string[] s3URLs
    }
```

## ğŸ”Œ API Design

### Initial APIs

- `POST /presigned_url` - Get presigned URL for direct S3 upload
  - Request: VideoMetadata
  - Response: Presigned URL
- `GET /videos/{videoId}` - Get video metadata
  - Response: VideoMetadata with manifest file URLs

## ğŸ¬ Video Storage & Streaming Fundamentals

### Key Concepts

- **Video Codec**: Compresses/decompresses video (H.264, H.265, VP9, AV1)
- **Video Container**: File format storing video data and metadata
- **Bitrate**: Data transmitted per time (affects quality)
- **Manifest Files**: Index files listing video segments in different formats
  - Primary manifest: Lists all available video versions
  - Media manifest: Lists segments for specific format

## ğŸ“¤ Video Upload Flow

### Evolution of Storage Approaches

#### âŒ Bad: Store Raw Video

- Different devices need different formats
- No post-processing = incompatible playback

#### âœ… Good: Store Different Formats

- Post-process into multiple formats
- Still stores entire video files

#### âœ…âœ… Great: Store Formatted Segments

- Split videos into small segments (few seconds each)
- Convert each segment to multiple formats
- Enables adaptive bitrate streaming

### Upload Architecture

```mermaid
graph TB
    Client[Client]
    Gateway[API Gateway & Load Balancer]
    VideoService[Video Service]
    S3[(S3)]
    MetadataDB[(Video Metadata DB Cassandra)]
    ProcessingService[Video Processing Service]

    Client --> Gateway
    Gateway --> VideoService
    VideoService --> Client
    VideoService --> MetadataDB
    Client --> S3
    S3 --> ProcessingService
    ProcessingService --> S3
    ProcessingService --> MetadataDB

```

## ğŸ“º Video Streaming Flow

### Evolution of Streaming Approaches

#### âŒ Bad: Download Entire Video

- Long wait times (10GB = 13+ min on 100Mbps)
- Network disruption = lost progress
- Not true "streaming"

#### âœ… Good: Download Segments Incrementally

- Quick start (download first segment)
- Background downloading of next segments
- Doesn't adapt to network changes

#### âœ…âœ… Great: Adaptive Bitrate Streaming

- Dynamically adjusts quality based on network
- Uses manifest files to track segments
- Seamless quality transitions

### Streaming Process

1. Fetch VideoMetadata (includes manifest URL)
2. Download manifest file
3. Choose format based on network/settings
4. Download first segment
5. Play segment while downloading more
6. Adjust quality based on network conditions

```mermaid
sequenceDiagram
    participant Client
    participant VideoService
    participant Cache
    participant MetadataDB
    participant CDN
    participant S3

    Client->>VideoService: GET /video/{id}
    VideoService->>Cache: Check cache
    alt Cache Hit
        Cache-->>Client: Return metadata
    else Cache Miss
        VideoService->>MetadataDB: Query metadata
        MetadataDB-->>VideoService: Return metadata
        VideoService->>Cache: Store in cache
        VideoService-->>Client: Return metadata
    end

    Client->>CDN: Get manifest file
    CDN->>S3: Fetch if not cached
    S3-->>CDN: Return manifest
    CDN-->>Client: Return manifest

    loop Adaptive Streaming
        Client->>Client: Check network conditions
        Client->>CDN: Request segment (chosen quality)
        CDN-->>Client: Return segment
        Client->>Client: Play segment
    end
```

## ğŸ”§ Deep Dives

### 1. Video Processing Pipeline (DAG)

```mermaid
graph TD
    Upload[Original Video Upload]
    Split[Video Splitter]
    T1[Transcode Segment 1]
    T2[Transcode Segment 2]
    T3[Transcode Segment N]
    Audio[Audio Processing]
    Transcript[Transcript Generation]
    Manifest[Build Manifest Files]
    Complete[Mark Upload Complete]

    Upload --> Split
    Split --> T1
    Split --> T2
    Split --> T3
    Split --> Audio
    Split --> Transcript

    T1 --> Manifest
    T2 --> Manifest
    T3 --> Manifest
    Audio --> Manifest
    Transcript --> Manifest

    Manifest --> Complete
```

**Key Points:**

- Use orchestrator (e.g., Temporal) for DAG management
- Parallel processing of segments
- Store temporary files in S3
- CPU-intensive transcoding distributed across workers

### 2. Resumable Uploads

**Implementation:**

1. Client divides video into chunks (5-10MB)
2. Each chunk has fingerprint hash
3. VideoMetadata tracks chunks with status
4. Client uploads chunks to S3
5. Lambda updates chunk status on S3 events
6. Resume by checking uploaded chunks

```mermaid
graph LR
    Client -->|Chunk Upload| S3
    S3 -->|Event| Lambda[Upload Monitor Lambda]
    Lambda -->|Update Status| MetadataDB[(Metadata DB)]
    Client -->|Check Status| MetadataDB
```

### 3. Scaling Strategies

#### Pattern: Handling Large Blobs

- Presigned URLs for direct S3 upload
- Multipart upload for resumability
- Bypass application servers for file transfer

#### Pattern: Scaling Reads

- **Problem**: Hot videos create DB bottlenecks
- **Solutions**:
  - Cassandra replication (multiple nodes serve same data)
  - LRU cache for popular video metadata
  - CDN for video segments and manifest files

### Final Architecture

```mermaid
graph TB
    Client[Client]
    Gateway[API Gateway<br/>& Load Balancer<br/>â€¢ Routing<br/>â€¢ Auth<br/>â€¢ Rate Limiting]
    VideoService[Video Service]
    S3[(S3<br/>â€¢ Original videos<br/>â€¢ Segments<br/>â€¢ Manifests)]
    MetadataDB[(Video Metadata DB<br/>Cassandra)]
    MetadataCache[(Metadata Cache<br/>LRU)]
    CDN[CDN<br/>Edge Servers]
    Lambda[Upload Monitor<br/>Lambda]

    subgraph Processing["Video Processing Service"]
        Splitter[Video Splitter]
        Transcoding[Transcoding Workers]
        AudioProc[Audio Processing]
        TranscriptGen[Transcript Generation]
        ManifestBuilder[Manifest Builder]
    end

    Client -->|Upload| Gateway
    Gateway --> VideoService
    VideoService -->|Presigned URL| Client
    Client -->|Direct Upload| S3

    S3 -->|Event| Lambda
    Lambda -->|Update chunks| MetadataDB

    S3 -->|Event| Processing
    Processing -->|Store segments| S3
    Processing -->|Update metadata| MetadataDB

    Client -->|Watch| Gateway
    Gateway --> VideoService
    VideoService --> MetadataCache
    MetadataCache --> MetadataDB

    Client -->|Stream| CDN
    CDN -->|Cache miss| S3
```

## ğŸ“Š Scale Calculations

### Storage

- 1M videos/day Ã— 365 days = 365M videos/year
- Partition by videoId in Cassandra

### Bandwidth

- 100M views/day
- CDN crucial for geographic distribution
- Cache popular content at edge

## ğŸ“ Interview Expectations by Level

### Mid-Level

- **Focus**: 80% breadth, 20% depth
- **Key Points**:
  - Define clear APIs and data model
  - Understand multipart upload
  - Basic segment-based streaming
  - Direct S3 interaction
  - One deep dive topic

### Senior

- **Focus**: 60% breadth, 40% depth
- **Key Points**:
  - Quick high-level design
  - Detailed video post-processing
  - Multipart upload for resumability
  - Adaptive bitrate streaming details
  - DAG-based processing pipeline

### Staff+

- **Focus**: 40% breadth, 60% depth
- **Key Points**:
  - Breeze through basics
  - Deep dive into complex scenarios
  - Trade-off analysis
  - Real-world experience examples
  - Alternative approaches (e.g., pipelined upload)

## ğŸ’¡ Additional Considerations

### Performance Optimizations

1. **Pipelined Upload**: Start processing while still uploading
2. **Predictive Caching**: Pre-cache likely next segments
3. **Geographic Sharding**: Store videos closer to expected audience

### Features to Discuss (if time)

- Resume playback position
- View count tracking (exact vs estimated)
- Live streaming capabilities
- Recommendation system integration
- Content delivery optimization

## ğŸ”‘ Key Takeaways

1. **Storage Strategy**: Segment-based storage enables adaptive streaming
2. **Upload Pattern**: Direct S3 upload with presigned URLs
3. **Processing**: DAG-based parallel processing for efficiency
4. **Streaming**: Adaptive bitrate with manifest files
5. **Scale**: CDN + caching for read-heavy workload
6. **Reliability**: Resumable uploads via chunking

## ğŸ“ Common Pitfalls to Avoid

- Don't store raw video only
- Don't route large files through application servers
- Don't forget about different device formats
- Don't neglect CDN for global users
- Don't process video sequentially (use parallel DAG)
- Don't ignore network variability (use adaptive streaming)

## ğŸ† Tips for Success

1. Start with simple design, iterate to complex
2. Clarify scope early (focus on video upload/stream)
3. Draw clear diagrams
4. Explain trade-offs explicitly
5. Use real numbers for scale calculations
6. Reference similar systems you've built
7. Ask clarifying questions about requirements

# YouTube System Design - Quick Revision Points

## ğŸ¯ Core Requirements

â€¢ **Upload videos** + **Stream videos** (focus on these two)
â€¢ High availability > consistency
â€¢ Support 10GB+ videos with resumable uploads
â€¢ Low latency streaming with adaptive quality
â€¢ Scale: 1M uploads/day, 100M views/day

## ğŸ“¤ Upload Flow - Key Evolution

â€¢ âŒ **Bad**: Store raw video â†’ incompatible formats
â€¢ âœ… **Good**: Store different formats â†’ still large files
â€¢ âœ…âœ… **Best**: Store formatted segments â†’ enables adaptive streaming

## ğŸ”‘ Upload Architecture Must-Haves

â€¢ **Presigned URLs** â†’ direct S3 upload (bypass servers)
â€¢ **Multipart upload** â†’ resumable (5-10MB chunks with fingerprints)
â€¢ **S3 events** â†’ trigger processing pipeline
â€¢ **DAG processing** â†’ parallel segment transcoding
â€¢ Store metadata in Cassandra (partition by videoId)

## ğŸ“º Streaming Flow - Key Evolution

â€¢ âŒ **Bad**: Download entire video â†’ long wait
â€¢ âœ… **Good**: Download segments incrementally â†’ quick start
â€¢ âœ…âœ… **Best**: Adaptive bitrate streaming â†’ adjusts to network

## ğŸ¬ Video Processing Essentials

â€¢ **Video segments**: Few seconds each
â€¢ **Multiple formats**: Different devices/bandwidths
â€¢ **Manifest files**:

- Primary manifest â†’ lists all formats
- Media manifest â†’ lists segments per format
  â€¢ **DAG pipeline**: Split â†’ Transcode (parallel) â†’ Audio â†’ Transcript â†’ Manifest

## ğŸ—ï¸ Architecture Components

â€¢ **API Gateway** â†’ routing, auth, rate limiting
â€¢ **Video Service** â†’ generates presigned URLs, serves metadata
â€¢ **S3** â†’ videos, segments, manifests
â€¢ **Cassandra** â†’ video metadata (high availability)
â€¢ **LRU Cache** â†’ popular video metadata
â€¢ **CDN** â†’ edge caching for segments/manifests
â€¢ **Lambda** â†’ monitor upload progress
â€¢ **Processing Service** â†’ DAG orchestrator (Temporal)

## ğŸ“Š Scaling Strategies

â€¢ **Reads**: CDN + LRU cache + Cassandra replication
â€¢ **Writes**: Direct S3 upload with presigned URLs
â€¢ **Hot videos**: Cache at multiple levels (metadata + segments)
â€¢ **Geographic**: CDN edge servers globally

## ğŸš€ Performance Optimizations

â€¢ **Pipelined upload**: Start processing while uploading
â€¢ **Predictive caching**: Pre-cache next segments
â€¢ **Adaptive quality**: Switch formats based on bandwidth
â€¢ **Parallel processing**: Transcode segments simultaneously

## âš ï¸ Critical Design Decisions

â€¢ **Segments over full files** â†’ enables streaming + adaptivity
â€¢ **Direct S3 upload** â†’ avoid server bottleneck
â€¢ **Manifest-based streaming** â†’ flexible format selection
â€¢ **DAG processing** â†’ efficient parallelization
â€¢ **Chunked uploads** â†’ resumability
â€¢ **CDN mandatory** â†’ global low-latency delivery

## ğŸ“ Level-Specific Focus

â€¢ **Mid-Level**: APIs, data model, basic streaming, one deep dive
â€¢ **Senior**: Processing pipeline, adaptive streaming, DAG details
â€¢ **Staff+**: Trade-offs, alternatives, real-world optimizations

## ğŸ”´ Common Mistakes to Avoid

â€¢ Routing large files through app servers
â€¢ Sequential video processing
â€¢ Ignoring different device formats
â€¢ Missing CDN for global scale
â€¢ Not supporting resumable uploads
â€¢ Storing only raw video

## ğŸ’¬ Key Talking Points

â€¢ "Segment-based storage enables adaptive streaming"
â€¢ "Presigned URLs bypass server bottleneck"
â€¢ "DAG allows parallel processing of segments"
â€¢ "Manifest files enable quality switching"
â€¢ "CDN + caching handles read-heavy load"
â€¢ "Chunking enables resumable uploads"

## ğŸ“ Quick API Reference

```
POST /presigned_url â†’ Get S3 upload URL
GET /videos/{id} â†’ Get video metadata + manifest URL
```

## ğŸ Interview Flow

1. Clarify requirements (focus on upload/stream)
2. Design data model (VideoMetadata with chunks)
3. Upload flow (presigned URL â†’ S3 â†’ processing)
4. Streaming flow (metadata â†’ manifest â†’ segments)
5. Scale discussion (CDN, caching, replication)
6. Deep dive (pick DAG/resumability/adaptive streaming)
